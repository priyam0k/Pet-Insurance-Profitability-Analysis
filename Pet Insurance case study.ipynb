{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet Insurance Profitability Analysis: A Ratemaking and Reserving Study\n",
    "\n",
    "## 1. Business Problem\n",
    "A hypothetical insurer, \"Pawtect Insurance,\" wants to launch a new pet insurance product. The company needs to establish a profitable pricing structure and estimate the required reserves for future claims. This project will develop a comprehensive actuarial analysis to address these needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Setup and Data Generation\n",
    "First, we'll import the necessary libraries and generate a synthetic dataset of 10,000 pet insurance policies. This data will serve as the basis for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Data Generation ---\n",
    "np.random.seed(42)\n",
    "num_policies = 10000\n",
    "\n",
    "data = {\n",
    "    'policy_id': range(1, num_policies + 1),\n",
    "    'pet_age': np.random.randint(1, 15, size=num_policies),\n",
    "    'pet_breed': np.random.choice(['Labrador', 'Poodle', 'Mixed', 'Bulldog', 'Beagle'], size=num_policies),\n",
    "    'location': np.random.choice(['Urban', 'Rural', 'Suburban'], size=num_policies),\n",
    "    'coverage_level': np.random.choice(['Basic', 'Standard', 'Premium'], size=num_policies),\n",
    "    'policy_year': np.random.randint(2020, 2024, size=num_policies)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def generate_claims(row):\n",
    "    base_freq = 0.1\n",
    "    base_sev = 500\n",
    "    age_factor = 1 + (row['pet_age'] * 0.05)\n",
    "    breed_factor = {'Labrador': 1.2, 'Poodle': 0.9, 'Mixed': 1.0, 'Bulldog': 1.5, 'Beagle': 0.8}.get(row['pet_breed'], 1)\n",
    "    location_factor = {'Urban': 1.3, 'Rural': 0.8, 'Suburban': 1.1}.get(row['location'], 1)\n",
    "    claim_freq = base_freq * age_factor * breed_factor * location_factor\n",
    "    claim_sev = base_sev * age_factor * breed_factor * location_factor\n",
    "    num_claims = np.random.poisson(claim_freq)\n",
    "    claims_incurred = sum(np.random.gamma(2, scale=claim_sev/2, size=num_claims))\n",
    "    return claims_incurred\n",
    "\n",
    "df['claims_incurred'] = df.apply(generate_claims, axis=1)\n",
    "df['annual_premium'] = 200 + (df['pet_age'] * 20) + df['claims_incurred'].apply(lambda x: max(0, x * np.random.uniform(0.3, 0.6)))\n",
    "df['claim_status'] = df['claims_incurred'].apply(lambda x: 'Paid' if x > 0 and np.random.rand() > 0.3 else 'Outstanding')\n",
    "\n",
    "print(\"--- Synthetic Data Generated ---\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ratemaking Analysis\n",
    "In this section, we'll perform an exploratory data analysis (EDA) to understand the key drivers of claims. We will then build a Generalized Linear Model (GLM) to create a risk-based pricing structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['claims_incurred'], bins=50, kde=True)\n",
    "plt.title('Distribution of Claims Incurred')\n",
    "plt.xlabel('Claims Incurred')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Loss ratio by pet breed\n",
    "loss_ratio_breed = df.groupby('pet_breed').agg({'claims_incurred': 'sum', 'annual_premium': 'sum'})\n",
    "loss_ratio_breed['loss_ratio'] = loss_ratio_breed['claims_incurred'] / loss_ratio_breed['annual_premium']\n",
    "print(\"Loss Ratio by Pet Breed:\")\n",
    "print(loss_ratio_breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss ratio by location\n",
    "loss_ratio_location = df.groupby('location').agg({'claims_incurred': 'sum', 'annual_premium': 'sum'})\n",
    "loss_ratio_location['loss_ratio'] = loss_ratio_location['claims_incurred'] / loss_ratio_location['annual_premium']\n",
    "print(\"\\nLoss Ratio by Location:\")\n",
    "print(loss_ratio_location)\n",
    "\n",
    "# Visualize loss ratios\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "loss_ratio_breed.plot(kind='bar', y='loss_ratio', ax=ax[0], title='Loss Ratio by Pet Breed')\n",
    "ax[0].set_ylabel('Loss Ratio')\n",
    "loss_ratio_location.plot(kind='bar', y='loss_ratio', ax=ax[1], title='Loss Ratio by Location')\n",
    "ax[1].set_ylabel('Loss Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Relationship between Pet Age and Claims Incurred\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df, x='pet_age', y='claims_incurred', alpha=0.5)\n",
    "plt.title('Claims Incurred vs. Pet Age')\n",
    "plt.xlabel('Pet Age (Years)')\n",
    "plt.ylabel('Claims Incurred ($)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM for Pricing\n",
    "df_glm = pd.get_dummies(df, columns=['pet_breed', 'location', 'coverage_level'], drop_first=True, dtype=int)\n",
    "df_glm['claim_count'] = df['claims_incurred'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Define features for the model\n",
    "features = ['pet_age', 'pet_breed_Labrador', 'pet_breed_Mixed', 'pet_breed_Poodle', 'location_Suburban', 'location_Urban']\n",
    "X = sm.add_constant(df_glm[features])\n",
    "\n",
    "# Frequency model (Poisson GLM)\n",
    "freq_model = sm.GLM(df_glm['claim_count'], X, family=sm.families.Poisson()).fit()\n",
    "print(\"--- GLM Frequency Model Results ---\")\n",
    "print(freq_model.summary())\n",
    "\n",
    "# Severity model (Gamma GLM)\n",
    "df_sev = df[df['claims_incurred'] > 0]\n",
    "df_glm_sev = df_glm.loc[df_sev.index]\n",
    "X_sev = sm.add_constant(df_glm_sev[features])\n",
    "\n",
    "sev_model = sm.GLM(df_sev['claims_incurred'], X_sev, family=sm.families.Gamma(link=sm.families.links.Log())).fit()\n",
    "print(\"--- GLM Severity Model Results ---\")\n",
    "print(sev_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reserving Analysis\n",
    "Here, we'll use the chain-ladder method to estimate the ultimate losses and calculate the Incurred But Not Reported (IBNR) reserves. This is a crucial step in ensuring the financial health of the insurer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample claims dataset for reserving\n",
    "claims_data = {\n",
    "    'accident_year': np.random.randint(2018, 2022, 1000),\n",
    "    'development_lag': np.random.randint(1, 5, 1000),\n",
    "    'paid_losses': np.random.randint(100, 5000, 1000)\n",
    "}\n",
    "claims_df = pd.DataFrame(claims_data)\n",
    "claims_df['paid_losses'] = claims_df.groupby(['accident_year', 'development_lag'])['paid_losses'].transform('sum')\n",
    "claims_df = claims_df.drop_duplicates()\n",
    "\n",
    "# Create loss triangle\n",
    "loss_triangle = claims_df.pivot(index='accident_year', columns='development_lag', values='paid_losses').fillna(0).astype(float)\n",
    "print(\"--- Loss Development Triangle ---\")\n",
    "print(loss_triangle)\n",
    "\n",
    "# Chain-Ladder Method\n",
    "def chain_ladder(triangle):\n",
    "    triangle = triangle.copy()\n",
    "    for i in range(triangle.shape[1] - 1):\n",
    "        col1 = triangle.iloc[:, i]\n",
    "        col2 = triangle.iloc[:, i+1]\n",
    "        factor = col2.sum() / col1.sum()\n",
    "        for j in range(triangle.shape[0] - (i + 1)):\n",
    "            triangle.iloc[j, i+1] = triangle.iloc[j, i] * factor\n",
    "    return triangle\n",
    "\n",
    "projected_triangle = chain_ladder(loss_triangle)\n",
    "print(\"--- Projected Loss Triangle (Chain-Ladder) ---\")\n",
    "print(projected_triangle)\n",
    "\n",
    "# Calculate ultimate losses and IBNR\n",
    "ultimate_losses = projected_triangle.iloc[:, -1]\n",
    "paid_to_date = loss_triangle.max(axis=1)\n",
    "ibnr = ultimate_losses - paid_to_date\n",
    "\n",
    "reserves = pd.DataFrame({'Ultimate Losses': ultimate_losses, 'Paid to Date': paid_to_date, 'IBNR': ibnr})\n",
    "print(\"\\n--- Reserve Estimation ---\")\n",
    "print(reserves)\n",
    "print(f\"\\nTotal IBNR Reserves: ${reserves['IBNR'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize IBNR Reserves\n",
    "plt.figure(figsize=(10, 6))\n",
    "reserves.plot(kind='bar', y=['Paid to Date', 'IBNR'], stacked=True, figsize=(10, 7))\n",
    "plt.title('Reserve Estimation (Paid-to-Date vs. IBNR)')\n",
    "plt.xlabel('Accident Year')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "This analysis provides a comprehensive actuarial framework for Pawtect Insurance's new pet insurance product. The ratemaking model offers a data-driven approach to pricing, while the reserving analysis ensures the company's solvency.\n",
    "The next steps include:\n",
    "1. Implementing the ratemaking model in the company's system.\n",
    "2. Conducting ongoing monitoring of claims experience and adjusting rates accordingly.\n",
    "3. Exploring additional factors that may influence claim costs (e.g., breed-specific risks).\n",
    "4. Developing strategies to manage potential adverse development in reserves.\n",
    "By following these recommendations, Pawtect can effectively price its pet insurance product and maintain financial stability over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Proposed Premium Structure and Performance Monitoring\n",
    "Based on the GLM results, we can construct a new risk-based premium for each policy. The pure premium is calculated as the product of the predicted claim frequency and the predicted claim severity. We will then add a loading for expenses and profit to arrive at the final proposed premium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict frequency and severity for each policy\n",
    "df_glm['predicted_freq'] = freq_model.predict(X)\n",
    "df_glm['predicted_sev'] = sev_model.predict(X)\n",
    "\n",
    "# Calculate pure premium\n",
    "df_glm['pure_premium'] = df_glm['predicted_freq'] * df_glm['predicted_sev']\n",
    "\n",
    "# Add a loading for expenses and profit margin. A 68% loading targets a loss ratio of 32%.\n",
    "loading_factor = 0.68\n",
    "df_glm['proposed_premium'] = df_glm['pure_premium'] / (1 - loading_factor)\n",
    "\n",
    "# Compare old vs. proposed premium\n",
    "print(\"--- Premium Comparison ---\")\n",
    "print(df[['annual_premium']].describe())\n",
    "print(df_glm[['proposed_premium']].describe())\n",
    "\n",
    "# Plot the distribution of proposed premiums\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_glm['proposed_premium'], bins=50, kde=True, color='green')\n",
    "plt.title('Distribution of Proposed Premiums')\n",
    "plt.xlabel('Proposed Premium')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare the distribution of old vs. proposed premiums\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.histplot(df['annual_premium'], bins=50, kde=True, color='blue', label='Original Premium')\n",
    "sns.histplot(df_glm['proposed_premium'], bins=50, kde=True, color='green', label='Proposed Premium')\n",
    "plt.title('Distribution of Original vs. Proposed Premiums')\n",
    "plt.xlabel('Annual Premium')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Profitability Analysis ---\n",
    "total_claims_incurred = df['claims_incurred'].sum()\n",
    "total_original_premium = df['annual_premium'].sum()\n",
    "total_proposed_premium = df_glm['proposed_premium'].sum()\n",
    "\n",
    "original_loss_ratio = total_claims_incurred / total_original_premium\n",
    "proposed_loss_ratio = total_claims_incurred / total_proposed_premium\n",
    "\n",
    "print(\"--- Profitability Comparison ---\")\n",
    "print(f\"Original Total Premium: ${total_original_premium:,.2f}\")\n",
    "print(f\"Proposed Total Premium: ${total_proposed_premium:,.2f}\")\n",
    "print(f\"Original Loss Ratio: {original_loss_ratio:.2%}\")\n",
    "print(f\"Proposed Loss Ratio: {proposed_loss_ratio:.2%}\")\n",
    "\n",
    "improvement = (original_loss_ratio - proposed_loss_ratio) / original_loss_ratio\n",
    "print(f\"\\nProjected Improvement in Loss Ratio: {improvement:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Incorporating External Data (Veterinary Cost Inflation)\n",
    "To enhance the accuracy of our models, we can incorporate external data, such as a veterinary cost inflation index. This will allow us to trend the historical claims data to a common cost level, leading to more accurate premium calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Incorporate Veterinary Cost Inflation ---\n",
    "vet_inflation = {2020: 0.05, 2021: 0.06, 2022: 0.07, 2023: 0.08}\n",
    "current_year = 2024\n",
    "\n",
    "df['inflation_factor'] = df['policy_year'].apply(lambda year: (1 + vet_inflation.get(year, 0))**(current_year - year))\n",
    "df['claims_incurred_trended'] = df['claims_incurred'] * df['inflation_factor']\n",
    "\n",
    "# --- Re-run GLM with Trended Data ---\n",
    "df_glm['claims_incurred_trended'] = df['claims_incurred_trended']\n",
    "sev_model_trended = sm.GLM(df_glm.loc[df_sev.index, 'claims_incurred_trended'], \n",
    "                             X_sev, \n",
    "                             family=sm.families.Gamma(link=sm.families.links.Log())).fit()\n",
    "\n",
    "print(\"--- GLM Severity Model Results (with Trended Data) ---\")\n",
    "print(sev_model_trended.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Advanced Reserving Model (Claim-Level Data)\n",
    "To further refine our reserve estimates, we can use claim-level data. This allows for a more granular analysis of claim development patterns. We'll simulate a claim-level dataset and then use it to build a more sophisticated reserving model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Simulate Claim-Level Data for Advanced Reserving ---\n",
    "num_claims = 5000\n",
    "claim_level_data = {\n",
    "    'claim_id': range(1, num_claims + 1),\n",
    "    'accident_year': np.random.randint(2018, 2022, size=num_claims),\n",
    "    'report_lag': np.random.geometric(p=0.5, size=num_claims),\n",
    "    'payment_lag': np.random.geometric(p=0.3, size=num_claims),\n",
    "    'claim_amount': np.random.lognormal(mean=6, sigma=1.5, size=num_claims)\n",
    "}\n",
    "claim_level_df = pd.DataFrame(claim_level_data)\n",
    "claim_level_df['report_year'] = claim_level_df['accident_year'] + claim_level_df['report_lag']\n",
    "claim_level_df['payment_year'] = claim_level_df['report_year'] + claim_level_df['payment_lag']\n",
    "\n",
    "# --- Basic Aggregation for Chain-Ladder (as a comparison) ---\n",
    "# This is a simplified version of what would be a more complex model\n",
    "paid_by_ay_dy = claim_level_df.groupby(['accident_year', 'payment_lag'])['claim_amount'].sum().unstack().fillna(0)\n",
    "print(\"--- Claim-Level Paid Loss Triangle ---\")\n",
    "print(paid_by_ay_dy.head())\n",
    "\n",
    "# --- Placeholder for a more advanced model ---\n",
    "# A true claim-level model would involve more complex methods like individual claim loss reserving models (e.g., GLMs or machine learning models on individual claim features).\n",
    "# For now, we'll just show the data structure.\n",
    "print(\"\\n--- Advanced Reserving Model (Placeholder) ---\")\n",
    "print(\"A more sophisticated model would use individual claim features to project ultimate losses.\")\n",
    "print(claim_level_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
